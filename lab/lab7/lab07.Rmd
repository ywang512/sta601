---
title: "STA 601 Lab 7: Bayesian (Generalized) Linear Regression Models"
author: "STA 601: Bayesian Inference and Modern Statistical Methods"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: rmdformats::readthedown
---

```{r setup, message=F, warning=F, echo=F}
#
require(tidyverse)
require(rstanarm)
require(magrittr)
library(tidyverse)
library(ggplot2)
require(loo)
require(bayesplot)
require(caret)
library(rstan)
require(HSAUR3)
#
ggplot2::theme_set(ggplot2::theme_bw())
knitr::opts_chunk$set(fig.align = 'center')
```

# Linear Regression

An experiment was run where clouds were seeded with silver iodide to examine whether increased rainfall would occur after the seeding. In the experiment, the "treatment variable" is whether or not the cloud was seeded, and the response is the amount of rainfall. The study data also include other covariates: the suitability criterion (sne), the percentage cloud cover in the experimental area, the prewetness/total rainfall in the target area one hour before seeding, the echomotion (stationary or moving), and the number of days after the first day of the experiment (time). 

```{r}
data("clouds", package = "HSAUR3")
head(clouds)
```

## Frequentist Approach

We will use a linear regression model to predict rainfall. We will include interactions of all covariates with seeding with the exception of the time variable. In the usual frequentist setting, we can fit the model as follows:

```{r}
ols <- lm(rainfall ~ seeding * (sne + cloudcover + prewetness + echomotion) + time,
          data = clouds)
coef(ols)
```

## Bayesian Approach

To run this model using Bayesian estimation, we can use the `rstanarm` package. This package wraps Stan code for common regression models. In contrast with the frequentist procedure, in Bayesian estimation we need to specify priors for our parameters. The `rstanarm` function equivalent of `lm()` is `stan_lm()`, and it requires a prior that is easy to specify but more difficult to conceptualize.

By now, we have learend that the OLS predictor for $\beta$ is $\hat{\boldsymbol{\beta}} = (\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\boldsymbol{Y}$ where $\boldsymbol{X}$ is the design matrix of centered predictors. As it turns out, the `lm` function in R performs QR decomposition on $\boldsymbol{X}$: $\boldsymbol{X} = \boldsymbol{Q}\boldsymbol{R}$ where $\boldsymbol{Q}$ is an orthogonal matrix ($\boldsymbol{Q}'\boldsymbol{Q} = \boldsymbol{I}$) and $\boldsymbol{R}$ is upper-triangular. So we can re-write the OLS estimators as: $$\hat{\boldsymbol{\beta}}= (\boldsymbol{X}'\boldsymbol{X})^{-1}\boldsymbol{X}'\boldsymbol{Y} = \boldsymbol{R}^{-1} \boldsymbol{Q}' \boldsymbol{Y}$$ 

***

### Exercise

Verify the equalities above

***

### Prior Specification: $\beta$ Coefficients

The `stan_glm()` function allows us to fit a linear model if we specify the `family` paramter to be `gaussian()`. The following code places independent Cauchy priors on the intercept and remaining predictors via the `prior_intercept` and `prior` arguments.

```{r, cache = T}
beta0.prior <- cauchy()
beta.prior <- cauchy()

stan.glm <- stan_glm(data = clouds,
                   formula = rainfall ~ seeding * (sne + cloudcover + prewetness + echomotion) + time,
                   family = gaussian(),
                   prior = beta.prior,
                   prior_intercept = beta0.prior,
                   refresh = 0)
```


***
### Exercise

How do the estimated coefficients compare in this glm model to those from the model fit using `lm`? How do the credible intervals / standard errors of the coeffients compare?

***

# Logistic Regression

Now that we've used the `stan_glm()` function, you might be wondering if you can fit other GLMs with a Bayesian model. We can. The `stan_glm()` function supports every link function that `glm()` supports. We will fit a logisitic regression model in the next example. 

Suppose we are interested in how an undergradate student's GRE, GPA, and college prestigiousness affect their admission into graduate school. The response variable is whether or not the student was admitted to graduate school.

```{r}
seed <- 196
admissions <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
## view the first few rows of the data
head(admissions)
admissions$rank <- factor(admissions$rank)
admissions$admit <- factor(admissions$admit)
admissions$gre <- scale(admissions$gre)
p <- 5
n <- nrow(admissions)
```

## Frequentist Approach

```{r}
freq.mod <- glm(admit ~. , data = admissions,
                family = binomial())
summary(freq.mod)
```

## Weakly Informative Prior: Normal

We have the choice of a logit or probit link. With `stan_glm()`, binomial models with a logit link function can typically be fit slightly faster than the identical model with a probit link because of how the two models are implemented in Stan. In the following code, we simply specify the chosen link, and set priors for the intercept and the predictor coefficients.

```{r, cache = T}
post1 <- stan_glm(admit ~ ., data = admissions,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = seed,
                 refresh = 0)
```

***

### Exercise

What do our choice of priors say about our beliefs? How do we interpret this prior? How do we interpret coefficients in a logistic model?

***

As always, it is good practice to run diagnostics to check model convergence. Here is a nice function that will allow you to do this without having to call many different functions:

```{r, eval = F}
launch_shinystan(post1)
```

Now we can look at posterior densities and estimates for the coefficients.

```{r, cache = T}
mcmc_areas(as.matrix(post1), prob = 0.95, prob_outer = 1)
round(coef(post1), 3)
round(posterior_interval(post1, prob = 0.95), 3)
```

## Posterior Predictive Checks

```{r, cache = T}
(loo1 <- loo(post1, save_psis = TRUE))
```

In the code chunk above, we assessed the strength of our model via its posterior preditive LOOCV. However as we know, this accuracy rate is quite meaningless unless we have something to compare it to. So let's create a baseline model with no predictors to compare to this first model:

```{r, cache = T}
post0 <- stan_glm(admit ~ 1, data = admissions,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = seed,
                 refresh = 0)
(loo0 <- loo(post0, save_psis = T))
rstanarm::compare_models(loo0, loo1)
```

***

### Exercise

Which model is better? Why?

***

Below, we compute posterior predictive probabilities of the linear predictor via the `posterior_linpred()` function provided in the rstanarm package. This function will extract posterior draws from the linear predictor. If we used a link function, then specifying the transform argument as `TRUE` will return the predictor as transformed via the inverse-link.

```{r, cache = T}
preds <- posterior_linpred(post1, transform=TRUE)
pred <- colMeans(preds)
```

We calculate these posterior predictive probabilities in order to determine the classification accuracy of our model. If the posterior probability of success for an individual is greater or equal to $0.5$, then we would predict that observation to be a success (and similarly for less than $0.5$). For each observation, we can compare the posterior prediction to the actual observed value. The proportion of times we correctly predict an individual (i.e. [prediction = 0 and observation = 0] or [prediction = 1 and observation = 1]) is our classification accuracy.

```{r}
pr <- as.integer(pred >= 0.5)
# have the students calculate this themselves?
round(mean(xor(pr,as.integer(admissions$admit==0))),3)
```

However, we should really be evaluating the classification accuracy of our model on unseen data. This can be done via a LOOCV approach or by using a test dataset. Here we use the former approach to illustrate the function `E_loo()`, which uses importance weights generated from the `loo()` function.

```{r, cache = T}
ploo=E_loo(preds, loo1$psis_object, type="mean", log_ratios = -log_lik(post1))$value
round(mean(xor(ploo>0.5,as.integer(admissions$admit==0))),3)
#round((mean(xor(ploo[admissions$admit==0]>0.5,as.integer(admissions$admit[admissions$admit==0])))+mean(xor(ploo[admissions$admit==1]<0.5,as.integer(admissions$admit[admissions$admit==1]))))/2,2)
```

## The Horseshoe Prior

In the case when we have more variables than observations, it will be difficult to achieve good estimates of the coefficients. To address this hurdle, we may consider alternative priors on the $\beta$s which place higher prior density on 0, effectively saying that those predictors should not be included in our final model. The horseshoe prior (`hs()`) is one such prior. In this dataset we have many samples and few covariates, so the horseshoe is not necessary. However, we will examine its effect on posterior inference of the $\beta$ coefficients.

```{r, cache = T}
p0 <- 2 # prior guess for the number of relevant variables
tau0 <- p0/(p-p0) * 1/sqrt(n) # recommended by Pilronen and Vehtari (2017)
hs_prior <- hs(df=1, global_df=1, global_scale=tau0)
post2 <- stan_glm(admit ~ ., data = admissions,
                 family = binomial(link = "logit"), 
                 prior = hs_prior, prior_intercept = normal(0,1),
                 seed = seed,
                 refresh = 0)

round(coef(post2), 3)
round(posterior_interval(post2, prob = 0.95), 3)
mcmc_areas(as.matrix(post2), prob = 0.95, prob_outer = 1)
```

***

### Exercise

How does posterior inference for the coefficients compare to when we used the weakly informative Normal prior above? Make sure to comment specifically on the posterior distribution for the coefficient for `rank2`.

***

How do the two models compare in terms of predictive performance?

```{r}
(loo2 <- loo(post2, save_psis = T))
rstanarm::compare_models(loo1, loo2)
```

*Adapted from this [tutorial](https://cran.r-project.org/web/packages/rstanarm/vignettes/lm.html)

*Also adapted from this [tutorial](https://avehtari.github.io/modelselection/diabetes.html)
